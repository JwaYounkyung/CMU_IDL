{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oxiZ42B4SwQ-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tests_hw4 import test_prediction, test_generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x5znxQhLSwRC"
      },
      "outputs": [],
      "source": [
        "# load all that we need\n",
        "\n",
        "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
        "devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)\n",
        "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
        "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
        "vocab = np.load('../dataset/vocab.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OZNrJ8XvSwRF"
      },
      "outputs": [],
      "source": [
        "# data loader\n",
        "\n",
        "class DataLoaderForLanguageModeling(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        self.dataset = dataset # TODO\n",
        "        self.batch_size = batch_size # TODO\n",
        "        self.shuffle = shuffle # TODO \n",
        "        \n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "            You may implement some of the techniques in https://arxiv.org/pdf/1708.02182.pdf\n",
        "            example: Variable length backpropagation sequences (Section 4.1)\n",
        "        \"\"\"\n",
        "        # 1. Randomly shuffle all the articles from the WikiText-2 dataset.\n",
        "        # 2. Concatenate all text in one long string.\n",
        "        # 3. Group the sequences into batches.\n",
        "        # 4. Run a loop that returns a tuple of (input, label) on every iteration with yield.\n",
        "\n",
        "        self.dataset = self.dataset.flatten()\n",
        "        self.dataset = self.dataset.tolist()\n",
        "        self.dataset = ' '.join(self.dataset)\n",
        "        self.dataset = self.dataset.split()\n",
        "        \n",
        "        \n",
        "        return 0\n",
        "        # yield (inputs, targets)\n",
        "\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zt-7YsTYSwRI"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "        TODO: Define your model here\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size:int, embedding_dim:int, hidden_size:int):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
        "        return 0\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kIvZOIfjSwRK"
      },
      "outputs": [],
      "source": [
        "# model trainer\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        # feel free to define a learning rate scheduler as well if you want\n",
        "        self.optimizer = None\n",
        "        self.criterion = None\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        \"\"\" \n",
        "            TODO: Define code for training a single batch of inputs\n",
        "            \n",
        "            :return \n",
        "                    (float) loss value\n",
        "        \"\"\"\n",
        "        raise NotImplemented\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.predict(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generate(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generate(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.predict(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        self.epochs += 1\n",
        "\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xPI7_kZRSwRN"
      },
      "outputs": [],
      "source": [
        "class TestLanguageModel:\n",
        "    def predict(inp, model):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "        \"\"\"\n",
        "        return 0\n",
        "        \n",
        "    def generate(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"        \n",
        "        return 0\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TiUrjbEjSwRQ"
      },
      "outputs": [],
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 2\n",
        "EMB_DIM = None\n",
        "HIDDEN_SIZE = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2HCVG5YISwRW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1672899841\n"
          ]
        }
      ],
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DbHH6zXTSwRa"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m Model(\u001b[39mlen\u001b[39m(vocab), embedding_dim\u001b[39m=\u001b[39mEMB_DIM, hidden_size\u001b[39m=\u001b[39mHIDDEN_SIZE)\n\u001b[1;32m      3\u001b[0m loader \u001b[39m=\u001b[39m DataLoaderForLanguageModeling(\n\u001b[1;32m      4\u001b[0m     dataset\u001b[39m=\u001b[39mdataset, \n\u001b[1;32m      5\u001b[0m     batch_size\u001b[39m=\u001b[39mBATCH_SIZE, \n\u001b[1;32m      6\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      9\u001b[0m     model\u001b[39m=\u001b[39mmodel, \n\u001b[1;32m     10\u001b[0m     loader\u001b[39m=\u001b[39mloader, \n\u001b[1;32m     11\u001b[0m     max_epochs\u001b[39m=\u001b[39mNUM_EPOCHS, \n\u001b[1;32m     12\u001b[0m     run_id\u001b[39m=\u001b[39mrun_id\n\u001b[1;32m     13\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
          ]
        }
      ],
      "source": [
        "loader = DataLoaderForLanguageModeling(\n",
        "    dataset=dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Model(len(vocab), embedding_dim=EMB_DIM, hidden_size=HIDDEN_SIZE)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, \n",
        "    loader=loader, \n",
        "    max_epochs=NUM_EPOCHS, \n",
        "    run_id=run_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7D8wTJkBSwRc"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "iter() returned non-iterator of type 'int'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m best_nll \u001b[39m=\u001b[39m \u001b[39m1e30\u001b[39m \n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m----> 3\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      4\u001b[0m     nll \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtest()\n\u001b[1;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m nll \u001b[39m<\u001b[39m best_nll:\n",
            "Cell \u001b[0;32mIn[8], line 32\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     31\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfor\u001b[39;00m batch_num, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader):\n\u001b[1;32m     33\u001b[0m     epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_batch(inputs, targets)\n\u001b[1;32m     34\u001b[0m epoch_loss \u001b[39m=\u001b[39m epoch_loss \u001b[39m/\u001b[39m (batch_num \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: iter() returned non-iterator of type 'int'"
          ]
        }
      ],
      "source": [
        "best_nll = 1e30 \n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2FmDqBCSwRf"
      },
      "outputs": [],
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipdbmqaGSwRh"
      },
      "outputs": [],
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.15 ('IDL')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "fffdd022f36d5bb59125dd46e70308eea6a9202a7f49dec9884d1f4a82c91f26"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
