{"cells":[{"cell_type":"markdown","metadata":{"id":"UR4qfYrVoO4v"},"source":["# Installs"]},{"cell_type":"markdown","metadata":{"id":"rd5aNaLVoR_g"},"source":["## wandb\n","\n","You will need to fetch your api key from wandb.ai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mA9qZoIDcx-h"},"outputs":[],"source":["!pip install wandb -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PiDduMaDIARE"},"outputs":[],"source":["import wandb\n","wandb.login(key=\"<replace with your API key here>\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4s52yBOvICPZ"},"outputs":[],"source":["run = wandb.init(\n","    name = \"early-submission\", ## Wandb creates random run names if you skip this field\n","    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    # run_id = ### Insert specific run id here if you want to resume a previous run\n","    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"hw3p2-ablations\", ### Project should be created in your wandb account \n","    config = config ### Wandb Config for your run\n",")"]},{"cell_type":"markdown","metadata":{"id":"ONgAWhqdoYy-"},"source":["## Levenshtein\n","\n","This may take a while"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SS7a7xeEoaV9"},"outputs":[],"source":["!pip install python-Levenshtein\n","!git clone --recursive https://github.com/parlance/ctcdecode.git\n","!pip install wget\n","%cd ctcdecode\n","!pip install .\n","%cd ..\n","\n","!pip install torchsummaryX"]},{"cell_type":"markdown","metadata":{"id":"IWVONJxCobPc"},"source":["## imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"78ZTCIXoof2f","outputId":"cf7c8f82-7aab-49ce-a68c-59b38e957cc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device:  cuda\n"]}],"source":["import torch\n","import random\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchsummaryX import summary\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","\n","import torchaudio.transforms as tat\n","\n","from sklearn.metrics import accuracy_score\n","import gc\n","\n","import zipfile\n","import pandas as pd\n","from tqdm import tqdm\n","import os\n","import datetime\n","\n","# imports for decoding and distance calculation\n","import ctcdecode\n","import Levenshtein\n","from ctcdecode import CTCBeamDecoder\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", device)"]},{"cell_type":"markdown","metadata":{"id":"gg3-yJ8tok34"},"source":["# Kaggle Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AdUelfGhom1m"},"outputs":[],"source":["!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n","!mkdir /root/.kaggle\n","\n","with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n","    f.write('{\"username\":\"\",\"key\":\"\"}') # TODO: Put your kaggle username & key here\n","\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74222,"status":"ok","timestamp":1665968038536,"user":{"displayName":"Abuzar Khan","userId":"14099904538183682197"},"user_tz":240},"id":"dSjBwfXeoq4B","outputId":"6515a9e6-3799-4293-d4b7-9e86bd61006c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading 11-785-f22-hw3p2.zip to /content\n","100% 8.89G/8.89G [01:12<00:00, 168MB/s]\n","100% 8.89G/8.89G [01:12<00:00, 131MB/s]\n"]}],"source":["!kaggle competitions download -c 11-785-f22-hw3p2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":175564,"status":"ok","timestamp":1665968214072,"user":{"displayName":"Abuzar Khan","userId":"14099904538183682197"},"user_tz":240},"id":"_ruxWP60LCQA","outputId":"42a894c0-09fa-4650-8ad2-e75557ec7d6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["11-785-f22-hw3p2.zip  ctcdecode  hw3p2\tsample_data\n"]}],"source":["'''\n","This will take a couple minutes, but you should see at least the following:\n","11-785-f22-hw3p2.zip  ctcdecode  hw3p2\n","'''\n","!unzip -q 11-785-f22-hw3p2.zip\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"R9v5ewZDMpYA"},"source":["# Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19070,"status":"ok","timestamp":1665970087208,"user":{"displayName":"Abuzar Khan","userId":"14099904538183682197"},"user_tz":240},"id":"4Cp-716IMZRd","outputId":"ea6dfaa1-32bc-4f57-fde7-07f99056ed18"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"2ORNHnSFroP0"},"source":["# Dataset and Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k0v7wHRWrqH6"},"outputs":[],"source":["# ARPABET PHONEME MAPPING\n","# DO NOT CHANGE\n","# This overwrites the phonetics.py file.\n","\n","CMUdict_ARPAbet = {\n","    \"\" : \" \", # BLANK TOKEN\n","    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n","    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n","    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n","    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n","    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n","    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n","    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n","    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n","    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"}\n","\n","CMUdict = list(CMUdict_ARPAbet.keys())\n","ARPAbet = list(CMUdict_ARPAbet.values())\n","\n","\n","PHONEMES = CMUdict\n","mapping = CMUdict_ARPAbet\n","LABELS = ARPAbet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eN2kcxwXLLBb"},"outputs":[],"source":["# You might want to play around with the mapping as a sanity check here"]},{"cell_type":"markdown","metadata":{"id":"agmNBKf4JrLV"},"source":["### Train Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afd0_vlbJmr_"},"outputs":[],"source":["class AudioDataset(torch.utils.data.Dataset):\n","\n","    # For this homework, we give you full flexibility to design your data set class.\n","    # Hint: The data from HW1 is very similar to this HW\n","\n","    #TODO\n","    def __init__(self): \n","        '''\n","        Initializes the dataset.\n","\n","        INPUTS: What inputs do you need here?\n","        '''\n","\n","        # Load the directory and all files in them\n","\n","        self.mfcc_dir = #TODO\n","        self.transcript_dir = #TODO\n","\n","        self.mfcc_files = #TODO\n","        self.transcript_files = #TODO\n","\n","        self.PHONEMES = PHONEMES\n","\n","        #TODO\n","        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n","        self.length = NotImplemented\n","        \n","        #TODO\n","        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n","        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n","\n","        #TODO\n","        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n","        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n","        '''\n","        You may decide to do this in __getitem__ if you wish.\n","        However, doing this here will make the __init__ function take the load of\n","        loading the data, and shift it away from training.\n","        '''\n","       \n","\n","    def __len__(self):\n","        \n","        '''\n","        TODO: What do we return here?\n","        '''\n","        raise NotImplemented\n","\n","    def __getitem__(self, ind):\n","        '''\n","        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n","\n","        If you didn't do the loading and processing of the data in __init__,\n","        do that here.\n","\n","        Once done, return a tuple of features and labels.\n","        '''\n","        \n","        raise NotImplemented\n","\n","        mfcc = ___ # TODO\n","        transcript = ___ # TODO\n","        return mfcc, transcript\n","\n","\n","    def collate_fn(self,batch):\n","        '''\n","        TODO:\n","        1.  Extract the features and labels from 'batch'\n","        2.  We will additionally need to pad both features and labels,\n","            look at pytorch's docs for pad_sequence\n","        3.  This is a good place to perform transforms, if you so wish. \n","            Performing them on batches will speed the process up a bit.\n","        4.  Return batch of features, labels, lenghts of features, \n","            and lengths of labels.\n","        '''\n","        # batch of input mfcc coefficients\n","        batch_mfcc = ___ # TODO\n","        # batch of output phonemes\n","        batch_transcript = ___ # TODO\n","\n","        # HINT: CHECK OUT -> pad_sequence (imported above)\n","        # Also be sure to check the input format (batch_first)\n","        batch_mfcc_pad = ___ # TODO\n","        lengths_mfcc = ___ # TODO \n","\n","        batch_transcript_pad = ___ # TODO\n","        lengths_transcript = ___ # TODO\n","\n","        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n","        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n","        #                  -> Would we apply transformation on the validation set as well?\n","        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n","        \n","        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n","        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n","\n","       "]},{"cell_type":"markdown","metadata":{"id":"hqDrxeHfJw4g"},"source":["### Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrLS1wfVJppA"},"outputs":[],"source":["# Test Dataloader\n","#TODO\n","class AudioDatasetTest(torch.utils.data.Dataset):\n","   pass"]},{"cell_type":"markdown","metadata":{"id":"Pt-veYcdL6Fe"},"source":["### Data - Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4icymeX1ImUN"},"outputs":[],"source":["BATCH_SIZE = 64 # Increase if your device can handle it\n","\n","transforms = [] # set of tranformations\n","# You may pass this as a parameter to the dataset class above\n","# This will help modularize your implementation\n","\n","root = '/content/hw3p2' "]},{"cell_type":"markdown","metadata":{"id":"NmuPk9J6L8dz"},"source":["### Data loaders"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_kG0gU2x4hH","outputId":"95a65754-500e-42ba-99c8-7b90bd6e1ff4"},"outputs":[{"data":{"text/plain":["202"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# get me RAMMM!!!! \n","import gc \n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4mzoYfTKu14s","outputId":"2197e432-f0c3-419a-eb88-0cb0d3af6e43"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch size:  64\n","Train dataset samples = 28539, batches = 446\n","Val dataset samples = 2703, batches = 43\n","Test dataset samples = 2620, batches = 41\n"]}],"source":["# Create objects for the dataset class\n","train_data = AudioDataset() #TODO\n","val_data = ___ # TODO : You can either use the same class with some modifications or make a new one :)\n","test_data = AudioDatasetTest() #TODO\n","\n","# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n","train_loader = #TODO\n","val_loader = #TODO\n","test_loader = #TODO\n","\n","print(\"Batch size: \", BATCH_SIZE)\n","print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n","print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n","print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXMtwyviKaxK","outputId":"8ae0460a-7492-422c-c1d1-2ea5bab2c0a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 1678, 15]) torch.Size([64, 212]) torch.Size([64]) torch.Size([64])\n"]}],"source":["# sanity check\n","for data in train_loader:\n","    x, y, lx, ly = data\n","    print(x.shape, y.shape, lx.shape, ly.shape)\n","    break "]},{"cell_type":"markdown","metadata":{"id":"Ly4mjUUUuJhy"},"source":["# Model Config"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZ-qQ_Sf-LIu","outputId":"ad9dc97d-1812-4bd4-cd13-50fed9d9034d"},"outputs":[{"data":{"text/plain":["43"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["OUT_SIZE = len(LABELS)\n","OUT_SIZE"]},{"cell_type":"markdown","metadata":{"id":"HLad4pChcuvX"},"source":["## Basic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQhvHr71GJfq"},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","class Network(nn.Module):\n","\n","    def __init__(self):\n","\n","        super(Network, self).__init__()\n","\n","        # Adding some sort of embedding layer or feature extractor might help performance.\n","        # self.embedding = ?\n","        \n","        # TODO : look up the documentation. You might need to pass some additional parameters.\n","        self.lstm = nn.LSTM(input_size = __, hidden_size = 256, num_layers = 1) \n","       \n","        self.classification = nn.Sequential(\n","            #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n","        )\n","\n","        \n","        self.logSoftmax = #TODO: Apply a log softmax here. Which dimension would apply it on ?\n","\n","    def forward(self, x, lx):\n","        #TODO\n","        # The forward function takes 2 parameter inputs here. Why?\n","        # Refer to the handout for hints\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"tUThsowyQdN7"},"source":["## INIT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGoiXd70tb5z"},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","model = Network().to(device)\n","summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"]},{"cell_type":"markdown","metadata":{"id":"IBwunYpyugFg"},"source":["# Training Config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MN82c3KpLup8"},"outputs":[],"source":["train_config = {\n","    \"beam_width\" : 2,\n","    \"lr\" : 2e-3,\n","    \"epochs\" : 50\n","    } # Feel free to add more items here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGoozH2nd6KB"},"outputs":[],"source":["#TODO\n","\n","\n","criterion = # Define CTC loss as the criterion. How would the losses be reduced?\n","# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n","# Refer to the handout for hints\n","\n","optimizer =  torch.optim.AdamW(...) # What goes in here?\n","\n","# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n","# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n","decoder = #TODO \n","\n","scheduler = #TODO\n","\n","# Mixed Precision, if you need it\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"markdown","metadata":{"id":"Jmc6_4eWL2Xp"},"source":["### Levenshtein"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHjnCDddL36E"},"outputs":[],"source":["# Use debug = True to see debug outputs\n","def calculate_levenshtein(h, y, lh, ly, decoder, labels, debug = False):\n","\n","    if debug:\n","        pass\n","        # print(f\"\\n----- IN LEVENSHTEIN -----\\n\")\n","        # Add any other debug statements as you may need\n","        # you may want to use debug in several places in this function\n","        \n","    # TODO: look at docs for CTC.decoder and find out what is returned here\n","    (...) = decoder.decode(h, seq_lens = lh)\n","\n","    batch_size = ___ # TODO\n","    distance = 0 # Initialize the distance to be 0 initially\n","\n","    for i in range(batch_size): \n","        # TODO: Loop through each element in the batch\n","        pass\n","\n","    # distance /= batch_size # TODO: Uncomment this, but think about why we are doing this\n","\n","    raise NotImplemented\n","    # return distance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GnTLL-5gMBrY"},"outputs":[],"source":["# ANOTEHR SANITY CHECK\n","\n","with torch.no_grad():\n","  for i, data in enumerate(train_loader):\n","      \n","      #TODO: \n","      # Follow the following steps, and \n","      # Add some print statements here for sanity checking\n","      \n","      #1. What values are you returning from the collate function\n","      #2. Move the features and target to <DEVICE>\n","      #3. Print the shapes of each to get a fair understanding \n","      #4. Pass the inputs to the model\n","            # Think of the following before you implement:\n","            # 4.1 What will be the input to your model?\n","            # 4.2 What would the model output?\n","            # 4.3 Print the shapes of the output to get a fair understanding \n","\n","      # Calculate loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n","      # Calculating the loss is not straightforward. Check the input format of each parameter\n","      \n","      loss = criterion(...) # What goes in here?\n","      print(f\"loss: {loss}\")\n","\n","      distance = calculate_levenshtein(out, y, out_lengths, ly, decoder, LABELS, debug = False)\n","      print(f\"lev-distance: {distance}\")\n","\n","      break # one iteration is enough"]},{"cell_type":"markdown","metadata":{"id":"6fLLj5KIMMOe"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"kH0RAbCaMl9a"},"source":["### Eval function\n","Writing a function to do one round of evaluations will help make your code more modular, you can, however, choose to skip this if you'd like it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nqLiAmkMMBc"},"outputs":[],"source":["torch.cuda.empty_cache()\n","def evaluate(data_loader, model):\n","    \n","    dist = 0\n","    loss = 0\n","    batch_bar = tqdm(total=len(data_loader), dynamic_ncols=True, leave=False, position=0, desc='Val') \n","    # TODO Fill this function out, if you're using it.\n","    \n","    return loss, dist"]},{"cell_type":"markdown","metadata":{"id":"qpYExu4vT4_g"},"source":["### Training Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tExvyl1BIdMC"},"outputs":[],"source":["# This is for checkpointing, if you're doing it over multiple sessions\n","\n","last_epoch_completed = 0\n","start = last_epoch_completed\n","end = epochs\n","best_val_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n","dist_freq = 1"]},{"cell_type":"markdown","metadata":{"id":"pGn17rLw9ChF"},"source":["Again, writing a train step might help you code be more modular. You may choose to skip this and write the whole thing out in the training loop below if you so wish."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_vH4QStLUjH8"},"outputs":[],"source":["def train_step(train_loader, model, optimizer, criterion, scheduler, scaler):\n","    \n","    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n","    train_loss = 0\n","\n","    for i, data in enumerate(train_loader):\n","\n","        # TODO: Fill this with the help of your sanity check\n","\n","        loss = criterion(...)\n","\n","        # HINT: Are you using mixed precision? \n","\n","        batch_bar.set_postfix(\n","            loss = f\"{train_loss/ (i+1):.4f}\",\n","            lr = f\"{optimizer.param_groups[0]['lr']}\"\n","        )\n","\n","        train_loss += loss\n","        batch_bar.update()\n","    \n","    batch_bar.close()\n","    train_loss /= ___ # TODO\n","\n","    return train_loss # And anything else you may wish to get out of this function"]},{"cell_type":"markdown","metadata":{"id":"MY69hgxUXhTI"},"source":["### Train Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JR43E28rM9Ak"},"outputs":[],"source":["torch.cuda.empty_cache()\n","gc.collect()\n","\n","#TODO: Please complete the training loop\n","\n","for epoch in range(train_config[\"epochs\"]):\n","\n","    # one training step\n","    # one validation step (if you want)\n","\n","    # HINT: Calculating levenshtein distance takes a long time. Do you need to do it every epoch?\n","    # Does the training step even need it? \n","\n","    # Where you have your scheduler.step depends on the scheduler you use.\n","\n","    \n","    # Use the below code to save models\n","    if val_dist < best_val_dist:\n","      #path = os.path.join(root_path, model_directory, 'checkpoint' + '.pth')\n","      print(\"Saving model\")\n","      torch.save({'model_state_dict':model.state_dict(),\n","                  'optimizer_state_dict':optimizer.state_dict(),\n","                  'val_dist': val_dist, \n","                  'epoch': epoch}, './checkpoint.pth')\n","      best_val_dist = val_dist\n","      wandb.save('checkpoint.pth')\n","    \n","\n","    # You may want to log some hyperparameters and results on wandb\n","    wandb.log()\n","\n","run.finish()"]},{"cell_type":"markdown","metadata":{"id":"M2H4EEj-sD32"},"source":["# Generate Predictions and Submit to Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2moYJhTWsOG-"},"outputs":[],"source":["#TODO: Make predictions\n","\n","# Follow the steps below:\n","# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n","# 2. Get prediction string by decoding the results of the beam decoder\n","\n","decoder_test = CTCBeamDecoder()\n","\n","def make_output(h, lh, decoder, LABELS):\n","  \n","    beam_results, beam_scores, timesteps, out_seq_len = decoder_test.decode() #TODO: What parameters would the decode function take in?\n","    batch_size = #What is the batch size\n","\n","    dist = 0\n","    preds = []\n","    for i in range(batch_size): # Loop through each element in the batch\n","\n","        h_sliced = #TODO: Obtain the beam results\n","        h_string = #TODO: Convert the beam results to phonemes\n","        preds.append(h_string)\n","    \n","    return preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d70dvu_lsMlv"},"outputs":[],"source":["#TODO:\n","# Write a function (predict) to generate predictions and submit the file to Kaggle\n","\n","torch.cuda.empty_cache()\n","predictions = predict(test_loader, model, decoder, LABELS)\n","import pandas as pd\n","\n","df = pd.read_csv('/content/hw3p2/test-clean/transcript/random_submission.csv')\n","df.label = predictions\n","\n","df.to_csv('submission.csv', index = False)\n","!kaggle competitions submit -c <competition> -f submission.csv -m \"I made it!\""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["UR4qfYrVoO4v","gg3-yJ8tok34","R9v5ewZDMpYA","Ly4mjUUUuJhy","HLad4pChcuvX","tUThsowyQdN7","IBwunYpyugFg","kH0RAbCaMl9a","qpYExu4vT4_g","MY69hgxUXhTI","M2H4EEj-sD32"],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 ('IDL')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13"},"vscode":{"interpreter":{"hash":"a11b6b5528ed2d3c7a3484bdfbfca06dbaf35091252ca8c2371102a3ed8c0f38"}}},"nbformat":4,"nbformat_minor":0}
